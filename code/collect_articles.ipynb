{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "collect_articles.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt5vVOCGeSDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install newspaper3k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwIdx_a6eoDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import newspaper\n",
        "from newspaper import Article\n",
        "import requests\n",
        "from newspaper import fulltext\n",
        "import pandas as pd\n",
        "from json import dumps, loads, JSONEncoder, JSONDecoder\n",
        "import pickle\n",
        "import csv\n",
        "import re\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V7HZbQqeeZr",
        "colab_type": "text"
      },
      "source": [
        "If article url contains date, this function extracts date from the url."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvcltZ3xehzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_date(url):\n",
        "  date_obj = re.findall(r'/(\\d{4})/(\\d{1,2})/(\\d{1,2})/', url)\n",
        "  if date_obj:\n",
        "    res = ['-'.join(tups) for tups in date_obj][0] \n",
        "    dt_date = datetime.strptime(res, '%Y-%m-%d').date()\n",
        "    return dt_date.isoformat()\n",
        "  return \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TIUTUxseZP5",
        "colab_type": "text"
      },
      "source": [
        "Function to remove filters. \n",
        "Returns false if filters are present\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKYW5xDRomq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_filters(article_url, filters):\n",
        "   return filters == '' or re.search(filters, article_url) is None\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7hW7RG9dzY8",
        "colab_type": "text"
      },
      "source": [
        "Function to collect all news articles and store in the list.\n",
        "Function paramters are as follows:\n",
        "1. url => url of news source\n",
        "2. filters => regex patterns to filter. eg. 'video|opinion'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB-XsR6tu1p1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def download_articles(url, filters): \n",
        "  filtered_websites = []\n",
        "  news_website = newspaper.build(url, memoize_articles=False, language=\"en\")\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  for article in news_website.articles:\n",
        "    \n",
        "    data_json = {}\n",
        "    web_article = Article(article.url)\n",
        "                \n",
        "    try:\n",
        "      web_article.download()\n",
        "      web_article.parse()\n",
        "      fulltext1 = requests.get(article.url).text\n",
        "      fulltext2 = fulltext(fulltext1)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    #check keywords in body as well as article url            \n",
        "    if (fulltext2.find('coronavirus') != -1 or fulltext2.find('covid-19') != -1 or fulltext2.find('sars-cov-2') != -1) or article.url.find('coronavirus') != -1 or article.url.find('covid-19') != -1 or article.url.find('sars-cov-2') != -1:\n",
        "      if (remove_filters(url, filters) == True):\n",
        "      #if (re.search(pattern, fulltext2) or re.search(pattern, web_article.url) ):\n",
        "        #print(web_article.url, web_article.title)\n",
        "      \n",
        "        data_json['url'] = web_article.url                                                                                                                    \n",
        "        data_json['publisher'] = web_article.source_url\n",
        "        if (web_article.publish_date is None or web_article.publish_date == \"\"):\n",
        "          print('No date')\n",
        "  \n",
        "          data_json['publish_date'] = extract_date(web_article.url)\n",
        "        else:\n",
        "          if(type(web_article.publish_date) == str):\n",
        "            pass\n",
        "\n",
        "          else:\n",
        "            data_json['publish_date'] = web_article.publish_date.date().isoformat()\n",
        "        remove_authors = ['May', 'Apr', 'Mar']\n",
        "                  \n",
        "        authors = list(web_article.authors)\n",
        "              \n",
        "        data_json['author'] = authors\n",
        "                  \n",
        "        data_json['title'] = web_article.title\n",
        "        print(article.url, data_json['title'])\n",
        "        data_json['image'] = web_article.top_image\n",
        "        data_json['body_text'] = fulltext2\n",
        "          \n",
        "        filtered_websites.append(data_json)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIMgpsnNoRVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_websites = download_articles('https://bipartisanreport.com/', '')\n",
        "print(filtered_websites)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BE-A-5HqBPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(filtered_websites)\n",
        "df.index.name = 'news_id'\n",
        "\n",
        "df = df.drop_duplicates(subset='title')\n",
        "df\n",
        "\n",
        "# You can change the name or make it custom\n",
        "df.to_csv(\"../dataset/news-dataset.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}